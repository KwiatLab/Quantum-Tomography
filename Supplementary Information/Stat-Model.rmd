---
title: "Stat Model"
output:
    pdf_document:
        extra_dependencies: ["braket","amsmath","mathtools","amssymb","multicol"]
---
\newcommand{\Like}{\mathcal{L}}
\newcommand{\EX}{\mathbb{E}}

# Introduction
Given the measurement data we wish to determine the best possible estimate for the quantum state. We can use [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to define the probability of the state given the data:

$$\text{Pr}(\rho|\text{Data}) = \frac{\text{Pr}(\text{Data}|\rho)\text{Pr}(\rho)}{\text{Pr}(\text{Data})}$$

We define these functions as the following

- Posterior:  $\text{Pr}(\rho|\text{Data})$
- Likelyhood: $\Like(\rho) = \text{Pr}(\text{Data}|\rho)$
- Prior:      $\text{Pr}(\rho)$
- Evidence:   $\text{Pr}(\text{Data})$

# Distribution of Counts
The counts of the measurements can be modeled by a [Poisson Distrobution](https://en.wikipedia.org/wiki/Poisson_distribution). With high enough counts we can approximate this with a [Normal Distrobution](https://en.wikipedia.org/wiki/Normal_distribution) using the [Central Limit Theorm](https://en.wikipedia.org/wiki/Central_limit_theorem). A important fact of the poisson distrobution is the variance is equal to the mean.

# 1det/qubit
For each measurement we have 1 count number. From the [Photonic State Tomography](http://research.physics.illinois.edu/QI/Photonics/tomography-files/amo_tomo_chapter.pdf) paper, the distribution of the counts follows:

$$n_i \sim \text{Poiss}(\mu_{i}) \xrightarrow{\text{CLT}}
       n_i \sim \text{Norm}(\mu_{i},\sigma_i)$$
$$\mu_{i} = I_0 I_i \text{Tr}\left( M'_i \rho \right) + a_i$$

### Variables:

- $n_i$: Number of counts on measurement i
- $\mu_i$: Expected number of counts on measurement i given $\rho$
- $i \in[1,m] \; \big| \; m = \text{Number of Measurements}$
- $\sigma_i^2$: Variance for the number of counts on measurement i given $\rho$
    -  $\sigma_i^2 = \mu_i$ from assuming a poisson distribution
- $I_0$: Is the overall intensity
- $I_i$: Is the relative intensity of measurement i given as an input. Default is 1
- $M'_i$: Is measurement i with cross talk correction
- $a_i$ : Is the predicted accidental counts for measurement i.

### log-Likelyhood

$$\Like(\rho) = \prod_{i=1}^m \text{Pr}(n_i|\mu_i)$$
$$\Like(\rho) = \prod_{i=1}^m \frac{1}{\sqrt{\sigma_i2\pi}}
                \exp\left( -\frac{(n_i-\mu_i)^2}{2\sigma_i^2} \right)$$

The $\frac{1}{\sqrt{\sigma_i2\pi}}$ is a normalization factor. Since we are maximizing this
function we can ignore this term

$$\Like(\rho) \propto \prod_{i=1}^m \exp\left( -\frac{(n_i-\mu_i)^2}{2\sigma_i^2} \right)$$

We can plug in $\sigma_i^2 = \mu_i$ and take the log of this function to get our log-Likelyhood function.

$$\text{log}(\Like(\rho)) = \sum_{i=1}^m -\frac{(n_i-\mu_i)^2}{2\mu_i^2}$$

# 2det/qubit
For each measurement a complete number of counts on all the possible outcomes. We define the following variables:

From the [Photonic State Tomography](http://research.physics.illinois.edu/QI/Photonics/tomography-files/amo_tomo_chapter.pdf) paper:

$$n_{ij} \sim \text{Poiss}(\mu_{ij}) \xrightarrow{\text{CLT}} n_{ij} \sim \text{Norm}(\mu_{ij},\sigma_{ij})$$
$$\mu_{ij} = I_0 I_i E_j \text{Tr}\left( M'_{ij} \rho \right) + a_{ij}$$

### Variables:

- $n_{ij}$: Number of counts on measurement i for detector j
- $\mu_{ij}$: Expected number of counts on measurement i, detector j  given $\rho$
- $i \in[1,m] \; \big| \; m = \text{Number of Measurements}$
- $j \in[1,k] \; \big| \; k = \text{Number of Detector Pairs}$
- $\sigma_{ij}^2$: Variance for the number of counts on measurement i, detector j given $\rho$
    -  $\sigma_{ij}^2 = \mu_{ij}$ from assuming a poisson distribution
- $I_0$: Is the overall intensity
- $I_i$: Is the relative intensity of measurement i given as an input. Default is 1
- $E_j$ Is the relative efficiency on the jth basis
- $M'_{ij}$: Is the jth basis of measurement i with cross talk correction


### log-Likelyhood

$$\Like(\rho) = \prod_{i=1}^m \prod_{j=1}^k  \text{Pr}(n_{ij}|\mu_{ij})$$
$$\Like(\rho) = \prod_{i=1}^m \prod_{j=1}^k  \frac{1}{\sqrt{\sigma_{ij}2\pi}}
                \exp\left( -\frac{(n_{ij}-\mu_{ij})^2}{2\sigma_{ij}^2} \right)$$

The $\frac{1}{\sqrt{\sigma_{ij}2\pi}}$ is a normalization factor. Since we are maximizing this
function we can ignore this term

$$\Like(\rho) \propto \prod_{i=1}^m \prod_{j=1}^k  \exp\left( -\frac{(n_{ij}-\mu_{ij})^2}{2\sigma_{ij}^2} \right)$$

We can plug in $\sigma_{ij}^2 = \mu_{ij}$ and take the log of this function to get our log-Likelyhood function.

$$\text{log}(\Like(\rho)) = \sum_{i=1}^m  \sum_{j=1}^k -\frac{(n_{ij}-\mu_{ij})^2}{2\mu_{ij}^2}$$


# Error Correction

## Accidental Correction

Accidental Correction is used for 2 detectors per qubit.

$$a_{ij} = \frac{W_j}{T_i * 10^9}\prod_{k=1}^{2}S_{ijk}$$

- $T_i$ Is the time in nanoseconds of measurement i given as an input. Default is 1
- $S_{ijk}$ Is the kth single count on measurement i given as an input. Default is 0
- $W_j$ Is the coincidence window duration for the jth basis as an input. Default is 0

## Crosstalk

$$M'_{i,j} = \sum_{j'=1}^{k} C_{j,j'} M_{i,j}$$

- $M'_{ij}$: Is the jth basis of measurement i with cross talk correction
- $M_{ij}$: Is the jth basis of measurement i.The state the target quantum state is projected on. These are pure states here represented as density matrices
- $C_{ij}$: The index at the jth row and j'th column of the crosstalk matrix
- $i \in[1,m] \; \big| \; m = \text{Number of Measurements}$
- $j,j' \in[1,k] \; \big| \; k = \text{Number of Detector Pairs, which is equal to the number of qubits squared.}$

# Lab setup example: 2 qubits with 2 det/qubit vs 1 det/qubit
We need to specify 2 states per measurement. The first is the state that the first qubit is projected onto when it ends up at detector 1. The second is the state that the second qubit is projected onto when it ends up at detector 2.

\begin{multicols}{2}
\textbf{2det/qubit}
\begin{enumerate}
  \item Det-pair 1 : 1-2
  \item Det-pair 2 : 1-4
  \item Det-pair 3 : 3-2
  \item Det-pair 4 : 3-4
\end{enumerate}

\columnbreak

\textbf{1det/qubit}
\begin{enumerate}
  \item Det-pair 1 : 1-2
\end{enumerate}

\end{multicols}


\begin{center}
    \includegraphics{DetectorPairs}
\end{center}



# Estimators

## Maximum Likelyhood Estimator(MLE)
The MLE estimator is found by maximizing the log of the likelyhood function

$$\rho_\text{MLE} =
\operatorname*{argmin}_{\rho} \text{log}(\Like(\rho))$$

## Bayesian Estimator(BME)
The Bayes estimator is the expected value of the posterior

$$\hat{\rho}_\text{BME} = \EX_\text{posterior}[\rho | \text{Data}]$$
$$\hat{\rho}_\text{BME} = \int\rho\text{Pr}(\rho|Data) d\rho$$
$$\hat{\rho}_\text{BME} =  \int\rho \frac{\text{Pr}(\text{Data}|\rho)\text{Pr}(\rho)}{\text{Pr}(\text{Data})} d\rho$$

We only care about an estimator that is proportional to the density matrix since we can normalize the matrix at the end.

$$\hat{\rho}_\text{BME} \propto \int\rho\text{Pr}(\text{Data}|\rho)\text{Pr}(\rho) d\rho$$
$$\hat{\rho}_\text{BME} \propto \EX_\text{prior}[\rho\text{Pr}(\text{Data}|\rho)]$$

## Monte Carlo Approximation of the Bayesian Estimator
$$\hat\rho_\text{BME}
=       \EX_\text{posterior}[\rho | \text{Data}]
\propto \EX_\text{prior}[\rho\text{Pr}(\text{Data}|\rho)]$$

- $\rho_i \sim \text{Pr}(\rho)$ is a random sample from the prior
$$\hat\rho_\text{BME} \propto \sum_i \rho_i\text{Pr}(\text{Data}|\rho)$$

- $\rho_i \sim \text{Pr}(\rho | \text{Data})$ is a random sample from posterior
$$\hat\rho_\text{BME} \propto \sum_i \rho_i$$