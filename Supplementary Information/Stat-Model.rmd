---
title: "Stat Model"
output:
    pdf_document:
        extra_dependencies: ["braket","amsmath","mathtools","amssymb","multicol"]
---
\newcommand{\Like}{\mathcal{L}}
\newcommand{\EX}{\mathbb{E}}

# Introduction
Given the measurement data we wish to determine the best possible estimate for the quantum state. We can use [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to define the probability of the state given the data:

$$\text{Pr}(\rho|\text{Data}) = \frac{\text{Pr}(\text{Data}|\rho)\text{Pr}(\rho)}{\text{Pr}(\text{Data})}$$

We define these functions as the following

- Posterior:  $\text{Pr}(\rho|\text{Data})$
- Likelyhood: $\Like(\rho) = \text{Pr}(\text{Data}|\rho)$
- Prior:      $\text{Pr}(\rho)$
- Evidence:   $\text{Pr}(\text{Data})$

# Distribution of Counts
The counts of the measurements can be modeled by a [Poisson Distrobution](https://en.wikipedia.org/wiki/Poisson_distribution). With high enough counts we can approximate this with a [Normal Distrobution](https://en.wikipedia.org/wiki/Normal_distribution) using the [Central Limit Theorm](https://en.wikipedia.org/wiki/Central_limit_theorem). A important fact of the poisson distrobution is the variance is equal to the mean.

# 1det/qubit
For each measurement we have 1 count number. We define the following variables:

- $n_i$: Number of counts on measurement i
    - $n_i \sim \text{Poiss}(\mu_{i}) \xrightarrow{\text{CLT}}
       n_i \sim \text{Norm}(\mu_{i},\sigma_i)$
    - $i \in[1,m] \; \big| \; m = \text{Num Measurements}$
- $\sigma_i^2$: Variance for the number of counts on measurement i given $\rho$
    -  $\sigma_i^2 = \mu_i$ from assuming a poisson distribution
- $\mu_i$: Expected number of counts on measurement i given $\rho$

From the [Photonic State Tomography](http://research.physics.illinois.edu/QI/Photonics/tomography-files/amo_tomo_chapter.pdf) paper:

$$\mu_{i} = I_0 I_i \text{Tr}\left( M'_i \rho \right) + a_i$$

### Variables:

- $I_0$: Is the overall intensity
- $I_i$: Is the relative intensity of measurement i given as an input. Default is 1
- $M'_i$: Is measurement i with cross talk correction
- $M_i$: Is measurement i
- $T_i$ Is the time of measurement i given as an input. Default is 1
- $S_{ik}$ Is the kth single count on measurement i given as an input. Default is 0
- $W$ Is the coincidence window duration as an input. Default is 0
- $a_i = \frac{W\prod_{k=1}^{2}S_{ik}}{T_i}$: Is the predicted accidental counts for measurement i.

### log-Likelyhood

$$\Like(\rho) = \prod_{i=1}^m \text{Pr}(n_i|\mu_i)$$
$$\Like(\rho) = \prod_{i=1}^m \frac{1}{\sqrt{\sigma_i2\pi}}
                \exp\left( -\frac{(n_i-\mu_i)^2}{2\sigma_i^2} \right)$$

The $\frac{1}{\sqrt{\sigma_i2\pi}}$ is a normalization factor. Since we are maximizing this
function we can ignore this term

$$\Like(\rho) \propto \prod_{i=1}^m \exp\left( -\frac{(n_i-\mu_i)^2}{2\sigma_i^2} \right)$$

We can plug in $\sigma_i^2 = \mu_i$ and take the log of this function to get our log-Likelyhood function.

$$\text{log}(\Like(\rho)) = \sum_{i=1}^m -\frac{(n_i-\mu_i)^2}{2\mu_i^2}$$

# 2det/qubit
For each measurement a complete number of counts on all the possible outcomes. We define the following variables:

- $n_{ij}$: Number of counts on measurement i for detector j
    - $n_{ij} \sim \text{Poiss}(\mu_{ij}) \xrightarrow{\text{CLT}}
       n_{ij} \sim \text{Norm}(\mu_{ij},\sigma_{ij})$
    - $i \in[1,m] \; \big| \; m = \text{Num Measurements}$
    - $j \in[1,k] \; \big| \; k = \text{Num Detector Pairs}$

- $\sigma_{ij}^2$: Variance for the number of counts on measurement i, detector j given $\rho$
    -  $\sigma_{ij}^2 = \mu_{ij}$ from assuming a poisson distribution
- $\mu_{ij}$: Expected number of counts on measurement i, detector j  given $\rho$


From the [Photonic State Tomography](http://research.physics.illinois.edu/QI/Photonics/tomography-files/amo_tomo_chapter.pdf) paper:

$$\mu_{ij} = I_0 I_i E_j \text{Tr}\left( M'_{ij} \rho \right) + a_{ij}$$

### Variables:

- $I_0$: Is the overall intensity
- $I_i$: Is the relative intensity of measurement i given as an input. Default is 1
- $M'_{ij}$: Is the jth basis of measurement i with cross talk correction
- $M_{ij}$: Is the jth basis of measurement i
- $T_i$ Is the time of measurement i given as an input. Default is 1
- $S_{ijk}$ Is the kth single count on measurement i given as an input. Default is 0
- $W_j$ Is the coincidence window duration for the jth basis as an input. Default is 0
- $E_j$ Is the relative efficiency on the jth basis
- $a_{ij} = \frac{W_j\prod_{k=1}^{2}S_{ijk}}{T_i}$: Is the predicted accidental counts for measurement i.

### log-Likelyhood

$$\Like(\rho) = \prod_{i=1}^m \prod_{j=1}^k  \text{Pr}(n_{ij}|\mu_{ij})$$
$$\Like(\rho) = \prod_{i=1}^m \prod_{j=1}^k  \frac{1}{\sqrt{\sigma_{ij}2\pi}}
                \exp\left( -\frac{(n_{ij}-\mu_{ij})^2}{2\sigma_{ij}^2} \right)$$

The $\frac{1}{\sqrt{\sigma_{ij}2\pi}}$ is a normalization factor. Since we are maximizing this
function we can ignore this term

$$\Like(\rho) \propto \prod_{i=1}^m \prod_{j=1}^k  \exp\left( -\frac{(n_{ij}-\mu_{ij})^2}{2\sigma_{ij}^2} \right)$$

We can plug in $\sigma_{ij}^2 = \mu_{ij}$ and take the log of this function to get our log-Likelyhood function.

$$\text{log}(\Like(\rho)) = \sum_{i=1}^m  \sum_{j=1}^k -\frac{(n_{ij}-\mu_{ij})^2}{2\mu_{ij}^2}$$

# Lab setup example: 2 qubits with 2 det/qubit vs 1 det/qubit
We need to specify 2 states per measurement. The first is the state that the first qubit is projected onto when it ends up at detector 1. The second is the state that the second qubit is projected onto when it ends up at detector 2.

\begin{multicols}{2}
\textbf{2det/qubit}
\begin{enumerate}
  \item Det-pair 1 : 1-2
  \item Det-pair 2 : 1-4
  \item Det-pair 3 : 3-2
  \item Det-pair 4 : 3-4
\end{enumerate}

\columnbreak

\textbf{1det/qubit}
\begin{enumerate}
  \item Det-pair 1 : 1-2
\end{enumerate}

\end{multicols}


\begin{center}
    \includegraphics{DetectorPairs}
\end{center}



# Estimators

## Maximum Likelyhood Estimator(MLE)
The MLE estimator is found by maximizing the log of the likelyhood function

$$\rho_\text{MLE} =
\operatorname*{argmin}_{\rho} \text{log}(\Like(\rho))$$

## Bayesian Estimator(BME)
The Bayes estimator is the expected value of the posterior

$$\hat{\rho}_\text{BME} = \EX_\text{posterior}[\rho | \text{Data}]$$
$$\hat{\rho}_\text{BME} = \int\rho\text{Pr}(\rho|Data) d\rho$$
$$\hat{\rho}_\text{BME} =  \int\rho \frac{\text{Pr}(\text{Data}|\rho)\text{Pr}(\rho)}{\text{Pr}(\text{Data})} d\rho$$

We only care about an estimator that is proportional to the density matrix since we can normalize the matrix at the end.

$$\hat{\rho}_\text{BME} \propto \int\rho\text{Pr}(\text{Data}|\rho)\text{Pr}(\rho) d\rho$$
$$\hat{\rho}_\text{BME} \propto \EX_\text{prior}[\rho\text{Pr}(\text{Data}|\rho)]$$

## Monte Carlo Approximation of the Bayesian Estimator
$$\hat\rho_\text{BME}
=       \EX_\text{posterior}[\rho | \text{Data}]
\propto \EX_\text{prior}[\rho\text{Pr}(\text{Data}|\rho)]$$

- $\rho_i \sim \text{Pr}(\rho)$ is a random sample from the prior
$$\hat\rho_\text{BME} \propto \sum_i \rho_i\text{Pr}(\text{Data}|\rho)$$

- $\rho_i \sim \text{Pr}(\rho | \text{Data})$ is a random sample from posterior
$$\hat\rho_\text{BME} \propto \sum_i \rho_i$$